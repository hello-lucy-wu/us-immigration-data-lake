{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Immigration\n",
    "### Data Engineering Project\n",
    "\n",
    "According to the Gallup World Poll survey(2013-2016), the United States is one of the top desired destinations for potential migrants to move. Around 147 millions of people who participated in the survey worldwide would like to immigrate to the U.S. This project is to create a data lake to help people know more about the country demographics. In addition, the officers, who inspect and examine passengers at U.S. ports of entry, are able to retrieve insights over the i94 visitor arrival/departure records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the datasets used to build the data lake:\n",
    "* I94 Visitor Arrival/Departure Records: This data comes from the US National Tourism and Trade Office. You can read more about it [here](https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "* U.S. Cities Demographic Data: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "#### Here are the insights could be retrieved from I94 records:\n",
    "* Top 10 countries where visitors are from\n",
    "* Top 3 busiest ports of entry\n",
    "* Number of visitors arrive at U.S ports per day\n",
    "\n",
    "#### Here are the questions might help people choose where to live:\n",
    "* Which group is the racial majority of each city? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "U.S. Cities Demographic Data has one column for total city poplulation, one column for race and one column for race count. However, the sum of race counts in each city is not equal to the total city polulation. With the assumption that people might self-identify themselves as members of different races, sum of race counts is used to calculate racial majority. We just want to have a general idea about community racial/ethnic composition in each city.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "A snowflake schema is created for optimizing queries on i94 visitor arrival/departure records analysis.\n",
    "* Fact table: \n",
    "    - visitor_arrival_departure_records \\\n",
    "    year: arrival year - 4 digits, Integer \\\n",
    "    month: arrival month, Integer \\\n",
    "    arrdate_date: arrival date, Date \\\n",
    "    depdate_date: departure date, Date \\\n",
    "    arrival_port_code: arrival port code, Integer \\\n",
    "    file_create_date: creating file date, String \\\n",
    "    cicid: visitor client id, Integer \n",
    "\n",
    "    \n",
    "* Dimension tables: \n",
    "    - visitors \\\n",
    "    cicid: visitor client id, Integer \\\n",
    "    from_country_code: code for the country that visitor comes from, Integer \\\n",
    "    arrival_transportation_code: arrival transportation code, Integer \\\n",
    "    address_state: state code, String \\\n",
    "    occup as occupation: visitor's occupation, String \\\n",
    "    birthyear: visitor's birthyear, Integer \\\n",
    "    gender: visitor's gender, String \\\n",
    "    visit_purpose_code: code for the visiting purpose, Integer \\\n",
    "    visatype: type of visa, String\n",
    "    - country_codes \\\n",
    "    code: country code, Integer \\\n",
    "    name: country name, String\n",
    "    - port_codes \\\n",
    "    code: port code, Integer \\\n",
    "    location: port of entry location, String \n",
    "    - visit_purpose_codes (1 - Business, 2 - Pleasure, 3 - Student)\\\n",
    "    code: code, Integer \\\n",
    "    purpose: purpose of visiting U.S., String \n",
    "    - arrival_transportation_codes (1 - Air, 2 - Sea, 3 - Land, 9 - Unknown)\\\n",
    "    code: transportation code, Integer \\\n",
    "    transportation, String\n",
    "\n",
    "\n",
    "A star schema is created for optimizing queries on U.S. Cities Demographic analysis.\n",
    "* Fact table:\n",
    "    - cities \\\n",
    "    city: name of city, String \\\n",
    "    state_code: state abbreviation, String \\\n",
    "    total_population: Integer \\\n",
    "    race_code: Integer \\\n",
    "    race_count: Integer \n",
    "* Dimension table:\n",
    "    - state_codes \\\n",
    "    state_code: state abbreviation, String \\\n",
    "    state: state name, String\n",
    "    - race_codes \\\n",
    "    code: Integer \\\n",
    "    race: String\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "* Step 1: load i94 visitor arrival/departure raw records to staging table `border_arrival_departure_records_df` and change data type of some columns\n",
    "* Step 2: extract columns from staging table to create `visitor_arrival_departure_records` table and save it in parquet file(s)\n",
    "* Step 3: extract columns from staging table to create `visitors` table and save it in parquet file(s)\n",
    "* Step 4: load country codes txt file, create `country_codes` table, and save it in parquet file(s)\n",
    "* Step 5: load i94 port location mapping txt file, create `port_codes` table, and save it in parquet file(s)\n",
    "* Step 6: load arrival transportation codes txt file and create `arrival_transportation_codes` table\n",
    "* Step 7: load visit purpose codes txt file and create `visit_purpose_codes` table\n",
    "* Step 8: load U.S. cities demographics csv file to staging table `us_cities_demographics_staging_df` and change data type of some columns\n",
    "* Step 9: extract columns from staging table to create `race_codes` table and save it in parquet file(s)\n",
    "* Step 10: extract columns from staging table to create `us_cities_demographics` table and save it in parquet file(s)\n",
    "* Step 11: extract columns from staging table to create `state_codes` table and save it in parquet file(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long, TimestampType as Ts\n",
    "from pyspark.sql.functions import udf, col, row_number, length\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.5\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 1: load i94 visitor arrival/departure raw records to staging table `border_arrival_departure_records_df` and change data type of some columns\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_arrival_departure_records_in_parquet = './raw/sas_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_arrival_departure_records_df = spark.read.parquet(border_arrival_departure_records_in_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('cicid', border_arrival_departure_records_df[\"cicid\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94yr', border_arrival_departure_records_df[\"i94yr\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94mon', border_arrival_departure_records_df[\"i94mon\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94res', border_arrival_departure_records_df[\"i94res\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94mode', border_arrival_departure_records_df[\"i94mode\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94bir', border_arrival_departure_records_df[\"i94bir\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('i94visa', border_arrival_departure_records_df[\"i94visa\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('biryear', border_arrival_departure_records_df[\"biryear\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('arrdate', border_arrival_departure_records_df[\"arrdate\"].cast(Int()))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('depdate', border_arrival_departure_records_df[\"depdate\"].cast(Int()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "\n",
    "add_days = udf (lambda x: datetime.strptime('19600101', '%Y%m%d').date() + timedelta(days=x) if x is not None else None, Date())\n",
    "\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('arrdate_date', add_days(col('arrdate')))\n",
    "border_arrival_departure_records_df = border_arrival_departure_records_df.withColumn('depdate_date', add_days(col('depdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 2: extract columns from staging table to create `visitor_arrival_departure_records` table and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['i94yr as year', 'i94mon as month', 'arrdate_date', 'depdate_date', 'i94port as arrival_port_code', 'dtadfile as file_create_date', 'cicid']\n",
    "arrival_departure_records_df = border_arrival_departure_records_df.selectExpr(*cols)\n",
    "arrival_departure_records_df.write.partitionBy('year', 'month').parquet('./visitor-arrival-departure-records', 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+------------+-----------------+----------------+-------+\n",
      "|year|month|arrdate_date|depdate_date|arrival_port_code|file_create_date|  cicid|\n",
      "+----+-----+------------+------------+-----------------+----------------+-------+\n",
      "|2016|    4|  2016-04-30|  2016-05-08|              LOS|        20160430|5748517|\n",
      "|2016|    4|  2016-04-30|  2016-05-17|              LOS|        20160430|5748518|\n",
      "+----+-----+------------+------------+-----------------+----------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrival_departure_records_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 3: extract columns from staging table to create `visitors` table and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['cicid', 'i94res as from_country_code', 'i94mode as arrival_transportation_code', 'i94addr as address_state', 'occup as occupation', 'biryear as birthyear', 'gender', 'i94visa as visit_purpose_code', 'visatype']\n",
    "visitors_df = border_arrival_departure_records_df.selectExpr(*cols)\n",
    "visitors_df.write.parquet('./visitors', 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------------------------+-------------+----------+---------+------+------------------+--------+\n",
      "|  cicid|from_country_code|arrival_transportation_code|address_state|occupation|birthyear|gender|visit_purpose_code|visatype|\n",
      "+-------+-----------------+---------------------------+-------------+----------+---------+------+------------------+--------+\n",
      "|5748517|              438|                          1|           CA|      null|     1976|     F|                 1|      B1|\n",
      "|5748518|              438|                          1|           NV|      null|     1984|     F|                 1|      B1|\n",
      "+-------+-----------------+---------------------------+-------------+----------+---------+------+------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visitors_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 4: load country codes txt file, create `country_codes` table, and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|name       |\n",
      "+----+-----------+\n",
      "|582 |MEXICO     |\n",
      "|236 |AFGHANISTAN|\n",
      "+----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_codes_in_txt = './raw/country-codes.txt'\n",
    "\n",
    "country_codes_schema = R([\n",
    "    Fld(\"code\",Int()),\n",
    "    Fld(\"name\",Str()),\n",
    "])\n",
    "\n",
    "country_codes_df = spark.read.csv(country_codes_in_txt, header=False, sep=';', schema=country_codes_schema)\n",
    "\n",
    "country_codes_df.write.parquet('./codes/country-codes', 'overwrite')\n",
    "\n",
    "country_codes_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 5: load i94 port location mapping txt file, create `port_codes` table, and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|code|location     |\n",
      "+----+-------------+\n",
      "|ALC |ALCAN, AK    |\n",
      "|ANC |ANCHORAGE, AK|\n",
      "+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_in_txt = './raw/i94-port-location-mapping.txt'\n",
    "\n",
    "ports_schema = R([\n",
    "    Fld(\"code\",Str()),\n",
    "    Fld(\"location\",Str()),\n",
    "])\n",
    "\n",
    "ports_df = spark.read.csv(ports_in_txt, header=False, sep=';', schema=ports_schema)\n",
    "\n",
    "ports_df.write.parquet('./codes/port-codes', 'overwrite')\n",
    "\n",
    "ports_df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 6: load arrival transportation codes txt file and create `arrival_transportation_codes` table\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|code|transportation|\n",
      "+----+--------------+\n",
      "|1   |Air           |\n",
      "|2   |Sea           |\n",
      "|3   |Land          |\n",
      "|9   |Unknown       |\n",
      "+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrival_transportation_codes_in_txt = './codes/arrival-transportation-codes.txt'\n",
    "\n",
    "arrival_transportation_codes_schema = R([\n",
    "    Fld(\"code\",Int()),\n",
    "    Fld(\"transportation\",Str()),\n",
    "])\n",
    "\n",
    "arrival_transportation_codes = spark.read.csv(arrival_transportation_codes_in_txt, header=False, sep=';', schema=arrival_transportation_codes_schema)\n",
    "\n",
    "arrival_transportation_codes.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 7: load visit purpose codes txt file and create `visit_purpose_codes` table\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|code|purpose |\n",
      "+----+--------+\n",
      "|1   |Business|\n",
      "|2   |Pleasure|\n",
      "|3   |Student |\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visit_purpose_codes_in_txt = './codes/visit-purpose-codes.txt'\n",
    "\n",
    "visit_purpose_codes_schema = R([\n",
    "    Fld(\"code\",Int()),\n",
    "    Fld(\"purpose\",Str()),\n",
    "])\n",
    "\n",
    "visit_purpose_codes = spark.read.csv(visit_purpose_codes_in_txt, header=False, sep=';', schema=visit_purpose_codes_schema)\n",
    "\n",
    "visit_purpose_codes.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 8: load U.S. cities demographics csv file to staging table `us_cities_demographics_staging_df` and change data type of some columns\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_us_cities_demographics_schema():    \n",
    "    schema = R([\n",
    "        Fld(\"city\",Str()),\n",
    "        Fld(\"state\",Str()),\n",
    "        Fld(\"median_age\",Dbl()),\n",
    "        Fld(\"male_population\",Int()),\n",
    "        Fld(\"female_population\",Int()),\n",
    "        Fld(\"total_population\",Int()),\n",
    "        Fld(\"number_of_veterans\",Int()),\n",
    "        Fld(\"number_of_foreign_born\",Int()),\n",
    "        Fld(\"average_household_size\",Dbl()),\n",
    "        Fld(\"state_code\",Str()),\n",
    "        Fld(\"race\",Str()),\n",
    "        Fld(\"count\",Int()),\n",
    "    ])\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_demographics_schema = generate_us_cities_demographics_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+------------------+-----+\n",
      "|city         |state        |median_age|male_population|female_population|total_population|number_of_veterans|number_of_foreign_born|average_household_size|state_code|race              |count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|Maryland     |33.8      |40601          |41862            |82463           |1562              |30908                 |2.6                   |MD        |Hispanic or Latino|25924|\n",
      "|Quincy       |Massachusetts|41.0      |44129          |49500            |93629           |4147              |32935                 |2.39                  |MA        |White             |58723|\n",
      "|Hoover       |Alabama      |38.5      |38040          |46799            |84839           |4819              |8229                  |2.58                  |AL        |Asian             |4759 |\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_cities_demographics_in_s3 = './raw/us-cities-demographics.csv'\n",
    "us_cities_demographics_staging_df = spark.read.csv(us_cities_demographics_in_s3, header=True, sep=';', schema=us_cities_demographics_schema)\n",
    "us_cities_demographics_staging_df.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 9: extract columns from staging table to create `race_codes` table and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_codes_df = us_cities_demographics_staging_df.select(['race']).distinct()\n",
    "windowSpec = W.orderBy('race')\n",
    "race_codes_df = race_codes.withColumn('code', row_number().over(windowSpec))\n",
    "race_codes_df.coalesce(1).write.parquet('./codes/race-codes', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----+\n",
      "|race                             |code|\n",
      "+---------------------------------+----+\n",
      "|American Indian and Alaska Native|1   |\n",
      "|Asian                            |2   |\n",
      "|Black or African-American        |3   |\n",
      "|Hispanic or Latino               |4   |\n",
      "|White                            |5   |\n",
      "+---------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "race_codes_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 10: extract columns from staging table to create us_cities_demographics table and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"us_cities_demographics_staging_df.*\", \"race_codes_df.code as race_code\"]\n",
    "us_cities_demographics_staging_df = us_cities_demographics_staging_df.join(race_codes_df, race_codes_df['race'] == us_cities_demographics_staging_df['race'], how = 'left').select(us_cities_demographics_staging_df['*'], race_codes_df['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city', 'state_code', 'total_population', 'code as race_code', 'count as race_count']\n",
    "us_cities_demographics_df = us_cities_demographics_staging_df.selectExpr(*cols)\n",
    "us_cities_demographics_df.coalesce(1).write.parquet('./cities', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------------+---------+----------+\n",
      "|city            |state_code|total_population|race_code|race_count|\n",
      "+----------------+----------+----------------+---------+----------+\n",
      "|Silver Spring   |MD        |82463           |4        |25924     |\n",
      "|Quincy          |MA        |93629           |5        |58723     |\n",
      "|Hoover          |AL        |84839           |2        |4759      |\n",
      "|Rancho Cucamonga|CA        |175232          |3        |24437     |\n",
      "|Newark          |NJ        |281913          |5        |76402     |\n",
      "|Peoria          |IL        |118661          |1        |1343      |\n",
      "|Avondale        |AZ        |80683           |3        |11592     |\n",
      "|West Covina     |CA        |108489          |2        |32716     |\n",
      "|O'Fallon        |MO        |85032           |4        |2583      |\n",
      "|High Point      |NC        |109828          |2        |11060     |\n",
      "+----------------+----------+----------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_cities_demographics_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Step 11: extract columns from staging table to create state_codes table and save it in parquet file(s)\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes_df = us_cities_demographics_staging_df.select(['state_code', 'state']).drop_duplicates() \n",
    "state_codes_df.coalesce(1).write.parquet('./codes/state-codes', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|state_code|         state|\n",
      "+----------+--------------+\n",
      "|        MT|       Montana|\n",
      "|        NC|North Carolina|\n",
      "|        MD|      Maryland|\n",
      "|        CO|      Colorado|\n",
      "+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_codes_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_if_table_has_records tests past\n"
     ]
    }
   ],
   "source": [
    "check_if_table_has_records = [\n",
    "    {'df': spark.read.parquet('./codes/state-codes'), 'table': 'state_codes'},\n",
    "    {'df': spark.read.parquet('./codes/country-codes'), 'table': 'country_odes'},\n",
    "    {'df': spark.read.parquet('./codes/race-codes'), 'table': 'race_odes'},\n",
    "    {'df': spark.read.parquet('./codes/port-codes'), 'table': 'port_odes'},\n",
    "    {'df': spark.read.parquet('./cities'), 'table': 'cities'},\n",
    "    {'df': spark.read.parquet('./visitors'), 'table': 'visitors'},\n",
    "    {'df': spark.read.parquet('./visitor-arrival-departure-records'), 'table': 'vistor_arrival_departure_records'},\n",
    "]\n",
    "\n",
    "for test in check_if_table_has_records:\n",
    "    if test['df'].count() == 0:\n",
    "        raise ValueError(f\"Data quality check failed. {test['table']} has no records\")\n",
    "print(\"check_if_table_has_records tests past\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Analytical Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################\n",
    "#\n",
    "# Top 10 countries where visitors are from\n",
    "#\n",
    "#########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|name          |count |\n",
      "+--------------+------+\n",
      "|UNITED KINGDOM|368421|\n",
      "|JAPAN         |249167|\n",
      "|CHINA, PRC    |185609|\n",
      "|FRANCE        |185339|\n",
      "|MEXICO        |179603|\n",
      "|GERMANY       |156613|\n",
      "|SOUTH KOREA   |136312|\n",
      "|BRAZIL        |134907|\n",
      "|AUSTRALIA     |112407|\n",
      "|INDIA         |107193|\n",
      "+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visitors_in_file = './visitors'\n",
    "country_codes_in_file = './codes/country-codes'\n",
    "\n",
    "visitors_df = spark.read.parquet(visitors_in_file)\n",
    "country_codes_df = spark.read.parquet(country_codes_in_file)\n",
    "\n",
    "country_code_count = visitors_df.groupby(visitors_df.from_country_code).count().sort(col(\"count\").desc()) \n",
    "top_10_countries_where_visitors_are_from = country_code_count.join(country_codes_df, country_code_count.from_country_code == country_codes_df.code, how = 'left').select(country_codes_df['name'], country_code_count['count']) \n",
    "\n",
    "top_10_countries_where_visitors_are_from.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Other Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons for building data lake with spark:\n",
    "* Spark has the ability to cache data across multiple nodes and transform data quickly.\n",
    "* Due to the fact that data are stored in its original format, it is very flexible to query any records or process the data with different approaches.\n",
    "* With schema-on-read feature, data format of data source can be flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In production environment, the script must be run by a cron job. It is even better to build a ELT pipeline with Airflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following scenarios:\n",
    " * If the data is increased by 100x: \\\n",
    "     I will use cloud service to run the script and save data in S3.\n",
    " * If the data populates a dashboard that must be updated on a daily basis by 7am every day: \\\n",
    "     Either set a cron job to run at 7am every day or create an Ariflow task to run at 7am every day.\n",
    " * If the database needed to be accessed by 100+ people: \\\n",
    "     I will create the data lake in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
